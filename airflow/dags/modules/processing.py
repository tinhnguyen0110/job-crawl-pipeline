import json
import logging
from datetime import datetime
import dateparser
import requests
from airflow.hooks.base import BaseHook
from airflow.models import Variable
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy import Table, Column, Integer, String, DateTime
from .database import JOBS_TABLE, PROCESSED_JOBS_TABLE
from sqlalchemy.dialects.postgresql import insert as pg_insert
import time
from airflow.hooks.base import BaseHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
logger = logging.getLogger(__name__)

# ---------------------- PROMPT TEMPLATES ----------------------
PROMPT_TITLE = """
You are given a job title in either English or Vietnamese.

Extract the following:

1. "job_title": The main job title, excluding any location or country code prefix (e.g., "VN", "US", "SG").

2. "seniority": Based on your understanding, infer the most likely seniority level (such as Intern, Junior, Mid-level, Senior, Lead, Manager, Head, Director, etc.). If unclear, leave it empty.

You may infer synonyms (e.g., "Associate" = Junior, "Mid-level" = Middle, "Staff" = Senior).

Keep the original language in "job_title".

Respond only in JSON format:

{{"job_title": "...", "seniority": "..."}}

Now analyze this title:

Input: {title}
"""

PROMPT_DESCRIPTION = """
You are an AI specializing in parsing information from job postings.
Your task is to extract structured data fields from the job description text below.

### FIELD DEFINITIONS
- **location:** The workplace location.
- **salary:** The salary range (e.g., "Up to 2000 USD", "Negotiable").
- **job_description:** Describes the tasks and responsibilities the candidate will perform. Often found under headers like "Job Description", "Your Responsibilities".
- **job_requirements:** The skills, experience, and qualifications the candidate must have. Often found under headers like "Job Requirements", "Your Skills and Experience", "Qualifications".
- **benefits:** The perks and benefits the company offers. Often found under headers like "Benefits", "Perks", "Why you'll love working here".

### FORMATTING RULES
- Keep all text in its original language; do not translate.
- Extract text verbatim; do not summarize or rephrase.
- For `job_description`, `job_requirements`, and `benefits`: If there are multiple points, join them with a newline character (`\\n`). Remove any leading bullet symbols (e.g., '-', '*', '+').
- If information for any field is not found, return an empty string `""`.
- Respond only with a single, valid JSON object.

### TEXT TO PARSE
{description}
"""
# Endpoint gi·ªù ƒë√¢y l√† DNS n·ªôi b·ªô c·ªßa Kubernetes
# D·∫°ng: http://<t√™n-service>.<t√™n-namespace>.svc.cluster.local:<port>/<path>
LITELLM_ENDPOINT = "http://litellm-service.platform-services:4000/chat/completions"
LITELLM_API_KEY = Variable.get("litellm-api-key", default_var="your_api_key_here")
VALID_MODELS = ["gemini-1.5-flash", "gemini-2.0-flash-lite", "gemini-1.5-pro-latest","gpt-4.1-nano","gpt-4.1-mini"]
if not LITELLM_API_KEY or LITELLM_API_KEY == "your_api_key_here":
    logger.warning("[LiteLLM] API key is missing or default is being used.")


# ---------------------- UTILITY FUNCTIONS ----------------------

def _call_litellm(prompt,model) -> dict:
    """
    H√†m g·ª≠i prompt ƒë·∫øn LiteLLM Gateway, s·ª≠ d·ª•ng c·∫•u h√¨nh hardcode t·∫°m th·ªùi.
    """
    try:

        headers = {"Authorization": f"Bearer {LITELLM_API_KEY}"}
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "response_format": {"type": "json_object"}
        }
        
        response = requests.post(LITELLM_ENDPOINT, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        
        llm_response_text = response.json()['choices'][0]['message']['content']
        return json.loads(llm_response_text)

    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå L·ªói m·∫°ng ho·∫∑c HTTP khi g·ªçi model: {e}")
        raise ValueError(f"API call failed: {e}") from e
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën trong h√†m _call_litellm: {e}")
        raise ValueError(f"API call failed: {e}") from e

def call_with_model_fallback(prompt, primary_model):
    tried_models = set()
    models_to_try = [primary_model] + [m for m in VALID_MODELS if m != primary_model]

    for model in models_to_try:
        if model in tried_models:
            continue
        try:
            return _call_litellm(prompt, model)
        except Exception as e:
            tried_models.add(model)
            continue

    raise RuntimeError("‚ùå T·∫•t c·∫£ c√°c model ƒë·ªÅu l·ªói.")

def _parse_post_date(time_posted, date_crawled_str):
    """H√†m ti·ªán √≠ch ƒë·ªÉ parse ng√†y, c·∫ßn chuy·ªÉn ƒë·ªïi date_crawled_str th√†nh datetime."""
    if not time_posted or time_posted.strip().lower() in ["kh√¥ng r√µ", ""]:
        return date_crawled_str
    
    # Chuy·ªÉn ƒë·ªïi date_crawled_str (l√† datetime object) v·ªÅ date object
    base_date = date_crawled_str.date() if isinstance(date_crawled_str, datetime) else date_crawled_str

    parsed_date = dateparser.parse(
        time_posted,
        languages=['vi', 'en'],
        settings={'RELATIVE_BASE': datetime.combine(base_date, datetime.min.time())}
    )
    return parsed_date if parsed_date else base_date


def process_jobs_and_update_db_callable(**kwargs):
    """
    H√†m ch√≠nh ƒë∆∞·ª£c Airflow g·ªçi, ƒë√£ ƒë∆∞·ª£c t√°i c·∫•u tr√∫c ho√†n ch·ªânh cho GKE.
    """
    params = kwargs.get("params", {})
    time_get_api = params.get("time_get_api", 10)  # M·∫∑c ƒë·ªãnh l√† 10 gi√¢y
    model_name_to_save = params.get("model_name") 
    

    if model_name_to_save not in VALID_MODELS:
        logger.warning(f"‚ùå Model '{model_name_to_save}' kh√¥ng h·ª£p l·ªá. model m·∫∑c ƒë·ªãnh l√† 'gemini-1.5-flash'")
        model_name_to_save = "gemini-1.5-flash"

    
    logger.info("B·∫Øt ƒë·∫ßu t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu th√¥ b·∫±ng LLM...")

    # --- 1. Thi·∫øt l·∫≠p k·∫øt n·ªëi Database ---
    ## TODO: Thay th·∫ø c√°c gi√° tr·ªã hardcode n√†y b·∫±ng Airflow Connection 'postgres_job_db'
    
    try:
        hook = PostgresHook(postgres_conn_id="cloud-sql")
        engine = hook.get_sqlalchemy_engine()
        with engine.connect() as conn:
            conn.execute("SELECT 1")  # Simple test query
            logger.info("[Postgres] Successfully connected to the Postgres database.")
    except Exception as e:
        logger.exception(f"[Postgres] Failed to connect to Postgres: {str(e)}")
    
    processed_count = 0
    
    try:
        with engine.connect() as connection:

            # --- 2. L·∫•y d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω ---
            # L·∫•y 3 jobs ch∆∞a x·ª≠ l√Ω ƒë·ªÉ l√†m vi·ªác trong m·ªôt l·∫ßn ch·∫°y
            select_query = text("SELECT id, title, company, description, time_posted, crawled_at FROM raw_jobs WHERE processed = FALSE;")
            jobs_to_process = connection.execute(select_query).mappings().all()

            if not jobs_to_process:
                logger.info("‚úÖ Kh√¥ng c√≥ job m·ªõi n√†o ƒë·ªÉ x·ª≠ l√Ω.")
                return "No new jobs to process."

            logger.info(f"T√¨m th·∫•y {len(jobs_to_process)} jobs ƒë·ªÉ x·ª≠ l√Ω b·∫±ng LLM.")
            
            # --- 3. X·ª≠ l√Ω t·ª´ng Job ---
            for job in jobs_to_process:
                job_id = job['id']
                logger.info(f"üîÑ ƒêang x·ª≠ l√Ω job ID {job_id}: {job['title']}")
                try:
                    # G·ªçi LLM ƒë·ªÉ x·ª≠ l√Ω title v√† description
                    title_result = call_with_model_fallback(PROMPT_TITLE.format(title=job['title']), model_name_to_save)
                    desc_result = call_with_model_fallback(PROMPT_DESCRIPTION.format(description=job['description']), model_name_to_save)
                    # T√≠nh to√°n ng√†y ƒëƒÉng
                    post_date = _parse_post_date(job['time_posted'], job['crawled_at'])

                    # T·ªïng h·ª£p d·ªØ li·ªáu cu·ªëi c√πng
                    final_data = {
                        "raw_job_id": job_id,
                        "job_title": title_result.get("job_title"),
                        "seniority": title_result.get("seniority"),
                        "company": job['company'],
                        "location": desc_result.get("location"),
                        "salary": desc_result.get("salary"),
                        "job_description": desc_result.get("job_description"),
                        "job_requirements": desc_result.get("job_requirements"),
                        "benefits": desc_result.get("benefits"),
                        "date_posted": post_date,
                        "model": model_name_to_save
                    }

                    # B·∫Øt ƒë·∫ßu transaction ƒë·ªÉ ƒë·∫£m b·∫£o to√†n v·∫πn d·ªØ li·ªáu
                    with connection.begin() as transaction:
                        try:
                            # 4. INSERT v√†o b·∫£ng processed_jobs
                            # S·ª¨A ·ªû ƒê√ÇY: D√πng pg_insert() thay v√¨ .insert()
                            insert_stmt = pg_insert(PROCESSED_JOBS_TABLE).values(final_data)

                            # Ph·∫ßn c√≤n l·∫°i gi·ªØ nguy√™n, b√¢y gi·ªù n√≥ s·∫Ω ho·∫°t ƒë·ªông
                            upsert_stmt = insert_stmt.on_conflict_do_update(
                                index_elements=['raw_job_id'],
                                set_={
                                    # C·∫≠p nh·∫≠t t·∫•t c·∫£ c√°c c·ªôt tr·ª´ c·ªôt id v√† raw_job_id
                                    col.name: getattr(insert_stmt.excluded, col.name) 
                                    for col in PROCESSED_JOBS_TABLE.c 
                                    if col.name not in ['id', 'raw_job_id']
                                }
                            )
                            connection.execute(upsert_stmt)

                            # 5. UPDATE c·ªù trong b·∫£ng jobs (gi·ªØ nguy√™n)
                            update_stmt = JOBS_TABLE.update().where(JOBS_TABLE.c.id == job_id).values(processed=True)
                            connection.execute(update_stmt)

                            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng job ID: {job_id}")
                            time.sleep(time_get_api)  # Gi·∫£ l·∫≠p th·ªùi gian x·ª≠ l√Ω
                            processed_count += 1
                        except Exception as e:
                            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω job ID {job_id}. L·ªói: {e}. B·ªè qua job n√†y.")
                            continue
                except Exception as e:
                    logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh x·ª≠ l√Ω job ID {job_id}: {e}", exc_info=True)
                    continue 
                
    except SQLAlchemyError as e:
        logger.error(f"‚ùå L·ªói k·∫øt n·ªëi ho·∫∑c th·ª±c thi SQL: {e}", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh x·ª≠ l√Ω: {e}", exc_info=True)
        raise

    return f"Ho√†n t·∫•t. ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {processed_count}/{len(jobs_to_process)} jobs."