import json
import logging
from datetime import datetime
import dateparser
import requests
from airflow.hooks.base import BaseHook
from airflow.models import Variable
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy import Table, Column, Integer, String, DateTime
from .database import JOBS_TABLE, PROCESSED_JOBS_TABLE
from sqlalchemy.dialects.postgresql import insert as pg_insert

logger = logging.getLogger(__name__)

# ---------------------- PROMPT TEMPLATES ----------------------
PROMPT_TITLE = """
Task: From the job title string, extract:
1. job_title: The name of the job, excluding any seniority level or location/country prefix (like VN, SG, US, etc.).
2. seniority: One of the following levels if present: Intern, Fresher, Junior, Middle, Senior, Lead, Manager, Head, Principal, Director, Chief. Leave empty if no level is found.

Respond in JSON format like:
{{"job_title": "...", "seniority": "..."}}

Now, analyze the following title:
Input: {title}
"""

PROMPT_DESCRIPTION = """
Extract the following information from this job description:

- location
- salary
- job_description (short summary)
- job_requirements
- benefits

If any field is missing, leave it empty. Respond in JSON format like:
{{
  "location": "...",
  "salary": "...",
  "job_description": "...",
  "job_requirements": "...",
  "benefits": "..."
}}

Description:
{description}
"""

def _call_litellm(prompt,model) -> dict:
    """
    H√†m g·ª≠i prompt ƒë·∫øn LiteLLM Gateway, s·ª≠ d·ª•ng c·∫•u h√¨nh hardcode t·∫°m th·ªùi.
    """
    try:
        # THAY ƒê·ªîI CHO GKE 1: C·∫•u h√¨nh hardcode
        # Endpoint gi·ªù ƒë√¢y l√† DNS n·ªôi b·ªô c·ªßa Kubernetes
        # D·∫°ng: http://<t√™n-service>.<t√™n-namespace>.svc.cluster.local:<port>/<path>
        endpoint = "http://litellm-proxy-deployment.model-serving:4000/chat/completions" # <-- THAY TH·∫æ B·∫∞NG T√äN SERVICE & NAMESPACE C·ª¶A B·∫†N
        api_key = "sk-SlN5E-XYkuaI02FUeZxV9g" # <-- ## TODO: Chuy·ªÉn v√†o Airflow Connection sau n√†y

        headers = {"Authorization": f"Bearer {api_key}"}
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "response_format": {"type": "json_object"}
        }
        
        response = requests.post(endpoint, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        
        llm_response_text = response.json()['choices'][0]['message']['content']
        return json.loads(llm_response_text)

    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå L·ªói m·∫°ng ho·∫∑c HTTP khi g·ªçi model: {e}")
        raise ValueError(f"API call failed: {e}") from e
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën trong h√†m _call_litellm: {e}")
        raise ValueError(f"API call failed: {e}") from e

def _parse_post_date(time_posted, date_crawled_str):
    """H√†m ti·ªán √≠ch ƒë·ªÉ parse ng√†y, c·∫ßn chuy·ªÉn ƒë·ªïi date_crawled_str th√†nh datetime."""
    if not time_posted or time_posted.strip().lower() in ["kh√¥ng r√µ", ""]:
        return date_crawled_str
    
    # Chuy·ªÉn ƒë·ªïi date_crawled_str (l√† datetime object) v·ªÅ date object
    base_date = date_crawled_str.date() if isinstance(date_crawled_str, datetime) else date_crawled_str

    parsed_date = dateparser.parse(
        time_posted,
        languages=['vi', 'en'],
        settings={'RELATIVE_BASE': datetime.combine(base_date, datetime.min.time())}
    )
    return parsed_date if parsed_date else base_date


def process_jobs_and_update_db_callable(**kwargs):
    """
    H√†m ch√≠nh ƒë∆∞·ª£c Airflow g·ªçi, ƒë√£ ƒë∆∞·ª£c t√°i c·∫•u tr√∫c ho√†n ch·ªânh cho GKE.
    """
    logger.info("B·∫Øt ƒë·∫ßu t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu th√¥ b·∫±ng LLM...")

    # --- 1. Thi·∫øt l·∫≠p k·∫øt n·ªëi Database ---
    ## TODO: Thay th·∫ø c√°c gi√° tr·ªã hardcode n√†y b·∫±ng Airflow Connection 'postgres_job_db'
    db_user = "postgres"
    db_pass = "123456"
    db_name = "job_db"
    db_host = "127.0.0.1"
    db_port = 5432

    db_url = f"postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
    engine = create_engine(db_url)
    
    model_name_to_save = "gemini-1.5-flash" # TODO: L·∫•y t·ª´ Airflow Variable sau n√†y

    processed_count = 0
    
    try:
        with engine.connect() as connection:

            # --- 2. L·∫•y d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω ---
            # L·∫•y 3 jobs ch∆∞a x·ª≠ l√Ω ƒë·ªÉ l√†m vi·ªác trong m·ªôt l·∫ßn ch·∫°y
            select_query = text("SELECT id, title, company, description, time_posted, crawled_at FROM raw_jobs WHERE processed = FALSE;")
            jobs_to_process = connection.execute(select_query).mappings().all()

            if not jobs_to_process:
                logger.info("‚úÖ Kh√¥ng c√≥ job m·ªõi n√†o ƒë·ªÉ x·ª≠ l√Ω.")
                return "No new jobs to process."

            logger.info(f"T√¨m th·∫•y {len(jobs_to_process)} jobs ƒë·ªÉ x·ª≠ l√Ω b·∫±ng LLM.")
            
            # --- 3. X·ª≠ l√Ω t·ª´ng Job ---
            for job in jobs_to_process:
                job_id = job['id']
                logger.info(f"üîÑ ƒêang x·ª≠ l√Ω job ID {job_id}: {job['title']}")
                try:
                    # G·ªçi LLM ƒë·ªÉ x·ª≠ l√Ω title v√† description
                    title_result = _call_litellm(PROMPT_TITLE.format(title=job['title']), model_name_to_save)
                    desc_result = _call_litellm(PROMPT_DESCRIPTION.format(description=job['description']), model_name_to_save)
                    # T√≠nh to√°n ng√†y ƒëƒÉng
                    post_date = _parse_post_date(job['time_posted'], job['crawled_at'])

                    # T·ªïng h·ª£p d·ªØ li·ªáu cu·ªëi c√πng
                    final_data = {
                        "raw_job_id": job_id,
                        "job_title": title_result.get("job_title"),
                        "seniority": title_result.get("seniority"),
                        "company": job['company'],
                        "location": desc_result.get("location"),
                        "salary": desc_result.get("salary"),
                        "job_description": desc_result.get("job_description"),
                        "job_requirements": desc_result.get("job_requirements"),
                        "benefits": desc_result.get("benefits"),
                        "date_posted": post_date,
                        "model": model_name_to_save
                    }

                    # B·∫Øt ƒë·∫ßu transaction ƒë·ªÉ ƒë·∫£m b·∫£o to√†n v·∫πn d·ªØ li·ªáu
                    with connection.begin() as transaction:
                        try:
                            # 4. INSERT v√†o b·∫£ng processed_jobs
                            # S·ª¨A ·ªû ƒê√ÇY: D√πng pg_insert() thay v√¨ .insert()
                            insert_stmt = pg_insert(PROCESSED_JOBS_TABLE).values(final_data)

                            # Ph·∫ßn c√≤n l·∫°i gi·ªØ nguy√™n, b√¢y gi·ªù n√≥ s·∫Ω ho·∫°t ƒë·ªông
                            upsert_stmt = insert_stmt.on_conflict_do_update(
                                index_elements=['raw_job_id'],
                                set_={
                                    # C·∫≠p nh·∫≠t t·∫•t c·∫£ c√°c c·ªôt tr·ª´ c·ªôt id v√† raw_job_id
                                    col.name: getattr(insert_stmt.excluded, col.name) 
                                    for col in PROCESSED_JOBS_TABLE.c 
                                    if col.name not in ['id', 'raw_job_id']
                                }
                            )
                            connection.execute(upsert_stmt)

                            # 5. UPDATE c·ªù trong b·∫£ng jobs (gi·ªØ nguy√™n)
                            update_stmt = JOBS_TABLE.update().where(JOBS_TABLE.c.id == job_id).values(processed=True)
                            connection.execute(update_stmt)

                            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng job ID: {job_id}")
                            processed_count += 1
                        except Exception as e:
                            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω job ID {job_id}. L·ªói: {e}. B·ªè qua job n√†y.")
                            continue
                except Exception as e:
                    logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh x·ª≠ l√Ω job ID {job_id}: {e}", exc_info=True)
                    continue 
                
    except SQLAlchemyError as e:
        logger.error(f"‚ùå L·ªói k·∫øt n·ªëi ho·∫∑c th·ª±c thi SQL: {e}", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh x·ª≠ l√Ω: {e}", exc_info=True)
        raise

    return f"Ho√†n t·∫•t. ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {processed_count}/{len(jobs_to_process)} jobs."