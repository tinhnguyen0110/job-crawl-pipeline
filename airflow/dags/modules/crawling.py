import os
import json
import logging
from datetime import datetime
from playwright.sync_api import sync_playwright, Page, TimeoutError as PlaywrightTimeoutError
from sqlalchemy import create_engine
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import SQLAlchemyError
from .database import METADATA, JOBS_TABLE

logger = logging.getLogger(__name__)

def _close_popups(page: Page):
    popup_close_selectors = [".dismiss", "button[aria-label='Close']"]
    for selector in popup_close_selectors:
        try:
            popup_button = page.locator(selector).first
            if popup_button.is_visible(timeout=500):
                logger.info(f"Ph√°t hi·ªán v√† ƒë√≥ng pop-up v·ªõi selector: '{selector}'")
                popup_button.click(timeout=2000)
                page.wait_for_timeout(500)
        except PlaywrightTimeoutError:
            pass
        except Exception as e:
            logger.warning(f"L·ªói kh√¥ng mong ƒë·ª£i khi c·ªë g·∫Øng ƒë√≥ng pop-up '{selector}': {e}")

def safe_click(page, job_card):

    # D·ªçn d·∫πp pop-up tr∆∞·ªõc khi click
    _close_popups(page)
    
    # Th·ª±c hi·ªán h√†nh ƒë·ªông click ch√≠nh
    logger.info("Th·ª±c hi·ªán click v√†o job card...")
    job_card.click()
    
    # D·ªçn d·∫πp pop-up m·ªôt l·∫ßn n·ªØa sau khi click
    _close_popups(page)


def crawl_jobstreet_to_local_callable(**kwargs):
    logical_date = kwargs["ds"]
    run_id = kwargs["run_id"]
    params = kwargs.get("params", {})
    start_url = params.get("start_url")
    if not start_url:
        raise ValueError("Kh√¥ng t√¨m th·∫•y 'start_url' trong params c·ªßa DAG.")

    safe_run_id = run_id.replace(":", "_").replace("+", "_")
    output_dir = f"/opt/airflow/data/{logical_date}"
    output_filename = f"{safe_run_id}.json"
    output_file_path = os.path.join(output_dir, output_filename)

    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl t·ª´ URL: {start_url}")
    logger.info(f"üìÅ File output s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {output_file_path}")
    os.makedirs(output_dir, exist_ok=True)

    scraped_data = []
    browser = None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(start_url)
            base_url = "https://www.jobstreet.vn"
            page_num = 1
            
            while page_num <= 2:
                logger.info(f"üîÑ ƒêang x·ª≠ l√Ω trang {page_num} t·∫°i URL: {page.url}")
                
                try:
                    page.wait_for_selector("div.job-card", timeout=10000)
                    jobs = page.query_selector_all("div.job-card")
                except Exception as e:
                    logger.error(f"Kh√¥ng t√¨m th·∫•y job cards: {e}")
                    break
                
                for job_card in jobs:
                    try:
                        job_card.click() # Ch√∫ng ta s·∫Ω b·ªè force=True v√† x·ª≠ l√Ω pop-up n·∫øu c√≥
                        page.wait_for_selector(".job-description-container", timeout=5000)

                        # B∆∞·ªõc 1: T√¨m c√°c ph·∫ßn t·ª≠ cha
                        header = page.query_selector(".sticky-container")
                        details_container = page.query_selector(".job-description-container")

                        # B∆∞·ªõc 2: T√¨m t·ª´ng ph·∫ßn t·ª≠ con v√† l·∫•y text m·ªôt c√°ch an to√†n
                        # TITLE
                        title_el = header.query_selector(".job-title.heading") if header else None
                        if title_el:
                            title = title_el.inner_text().strip()
                        else:
                            title = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'title'") # <-- Log ƒë·ªÉ debug

                        # COMPANY
                        company_el = header.query_selector(".company") if header else None
                        if company_el:
                            company = company_el.inner_text().strip()
                        else:
                            company = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'company'") # <-- Log ƒë·ªÉ debug

                        # TIME POSTED
                        time_el = header.query_selector(".listed-date") if header else None
                        if time_el:
                            time_posted = time_el.inner_text().strip()
                        else:
                            time_posted = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'time_posted'") # <-- Log ƒë·ªÉ debug

                        # DESCRIPTION
                        if details_container:
                            description = details_container.inner_text().strip()
                        else:
                            description = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'description'") # <-- Log ƒë·ªÉ debug
                        
                        # B∆∞·ªõc 3: Ghi d·ªØ li·ªáu
                        job_data = {
                            "title": title,
                            "company": company,
                            "time_posted": time_posted,
                            "description": description,
                            "crawled_at": datetime.now().isoformat()
                        }
                        
                        scraped_data.append(job_data)
                        logger.info(f"‚úÖ ƒê√£ crawl: {title} t·∫°i {company}")

                    except Exception as e:
                        # T·ªêI ∆ØU LOG L·ªñI: In ra c·∫£ URL ƒëang b·ªã l·ªói ƒë·ªÉ d·ªÖ ki·ªÉm tra l·∫°i b·∫±ng tay
                        current_url = page.url
                        logger.warning(f"‚ùå L·ªói khi x·ª≠ l√Ω m·ªôt job card t·∫°i URL: {current_url}. L·ªói: {e}")
            
                next_button = page.query_selector("a.next-page-button")
                if next_button and next_button.get_attribute("href"):
                    next_url = base_url + next_button.get_attribute("href")
                    logger.info(f"‚û°Ô∏è ¬†Chuy·ªÉn sang trang ti·∫øp theo: {next_url}")
                    page.goto(next_url)
                    page_num += 1
                else:
                    logger.info("‚úÖ H·∫øt trang, k·∫øt th√∫c crawl.")
                    break
                
    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh crawl: {e}", exc_info=True)
        raise
    finally:
        if browser: browser.close()

    if not scraped_data:
        logger.warning("Kh√¥ng crawl ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.")
        return None

    logger.info(f"Ghi {len(scraped_data)} jobs v√†o file {output_file_path}")
    with open(output_file_path, "w", encoding="utf-8") as f:
        json.dump(scraped_data, f, indent=2, ensure_ascii=False)
    return output_file_path

def load_raw_json_to_sql_callable(**kwargs):
    ti = kwargs['ti']
    input_file_path = ti.xcom_pull(task_ids='crawl_and_save_locally_task')
    if not input_file_path:
        logger.warning("Kh√¥ng c√≥ file ƒë·ªÉ x·ª≠ l√Ω. B·ªè qua.")
        return

    db_user = os.environ.get("DB_USER", "postgres")
    db_pass = os.environ.get("DB_PASS", "123456")
    db_name = os.environ.get("DB_NAME", "job_db")
    db_host = os.environ.get("DB_HOST", "host.docker.internal")
    db_port = os.environ.get("DB_PORT", "5432")
    db_url = f"postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
    engine = create_engine(db_url)
    
    with open(input_file_path, 'r', encoding='utf-8') as f:
        data_to_load = json.load(f)
    
    if not data_to_load:
        logger.info("File d·ªØ li·ªáu r·ªóng.")
        return

    unique_jobs = { (job.get('title'), job.get('company')): job for job in data_to_load if job.get('title') and job.get('company') }
    deduplicated_data = list(unique_jobs.values())

    if not deduplicated_data:
        logger.info("Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá sau khi kh·ª≠ tr√πng l·∫∑p.")
        return

    try:
        with engine.connect() as connection:
            logger.info("Ki·ªÉm tra v√† t·∫°o b·∫£ng 'jobs' n·∫øu c·∫ßn...")
            METADATA.create_all(engine, checkfirst=True) # Ch·ªâ t·∫°o b·∫£ng 'jobs'
            
            insert_stmt = pg_insert(JOBS_TABLE).values(deduplicated_data)
            do_nothing_stmt = insert_stmt.on_conflict_do_nothing(
                index_elements=['title', 'company']
            )
            
            with connection.begin() as transaction:
                result = connection.execute(do_nothing_stmt)
                logger.info(f"‚úÖ ƒê√£ ch√®n {result.rowcount} jobs th√¥ m·ªõi v√†o database.")
                
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ghi d·ªØ li·ªáu th√¥ v√†o database: {e}")
        raise