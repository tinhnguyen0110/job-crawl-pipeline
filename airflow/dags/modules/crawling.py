import os
import json
import logging
from datetime import datetime
from playwright.sync_api import sync_playwright, Page, TimeoutError as PlaywrightTimeoutError
from sqlalchemy import create_engine
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import SQLAlchemyError
from .database import METADATA, JOBS_TABLE
from .io_utils import save_data_to_destination
logger = logging.getLogger(__name__)

def _close_popups(page: Page):
    popup_close_selectors = [".dismiss", "button[aria-label='Close']"]
    for selector in popup_close_selectors:
        try:
            popup_button = page.locator(selector).first
            if popup_button.is_visible(timeout=500):
                logger.info(f"PhÃ¡t hiá»‡n vÃ  Ä‘Ã³ng pop-up vá»›i selector: '{selector}'")
                popup_button.click(timeout=1000)
                page.wait_for_timeout(500)
        except PlaywrightTimeoutError:
            pass
        except Exception as e:
            logger.warning(f"Lá»—i khÃ´ng mong Ä‘á»£i khi cá»‘ gáº¯ng Ä‘Ã³ng pop-up '{selector}': {e}")

def safe_click(page, job_card):

    _close_popups(page)
    job_card.click()
    _close_popups(page)


def crawl_jobstreet_to_local_callable(**kwargs):
    params = kwargs.get("params", {})
    start_url = params.get("start_url")
    if not start_url:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y 'start_url' trong params cá»§a DAG.")
    
    scraped_data = []

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(start_url)
            base_url = "https://www.jobstreet.vn"
            page_num = 1
            
            while page_num <= 2:
                logger.info(f"ðŸ”„ Äang xá»­ lÃ½ trang {page_num} táº¡i URL: {page.url}")
                
                try:
                    page.wait_for_selector("div.job-card", timeout=10000)
                    jobs = page.query_selector_all("div.job-card")
                except Exception as e:
                    logger.error(f"KhÃ´ng tÃ¬m tháº¥y job cards: {e}")
                    break
                
                for job_card in jobs:
                    try:
                        safe_click(page, job_card)
                        # job_card.click() # ChÃºng ta sáº½ bá» force=True vÃ  xá»­ lÃ½ pop-up náº¿u cÃ³
                        page.wait_for_selector(".job-description-container", timeout=5000)

                        # BÆ°á»›c 1: TÃ¬m cÃ¡c pháº§n tá»­ cha
                        header = page.query_selector(".sticky-container")
                        details_container = page.query_selector(".job-description-container")

                        # BÆ°á»›c 2: TÃ¬m tá»«ng pháº§n tá»­ con vÃ  láº¥y text má»™t cÃ¡ch an toÃ n
                        # TITLE
                        title_el = header.query_selector(".job-title.heading") if header else None
                        if title_el:
                            title = title_el.inner_text().strip()
                        else:
                            title = "N/A"
                            logger.warning("KhÃ´ng tÃ¬m tháº¥y selector cho 'title'") # <-- Log Ä‘á»ƒ debug

                        # COMPANY
                        company_el = header.query_selector(".company") if header else None
                        if company_el:
                            company = company_el.inner_text().strip()
                        else:
                            company = "N/A"
                            logger.warning("KhÃ´ng tÃ¬m tháº¥y selector cho 'company'") # <-- Log Ä‘á»ƒ debug

                        # TIME POSTED
                        time_el = header.query_selector(".listed-date") if header else None
                        if time_el:
                            time_posted = time_el.inner_text().strip()
                        else:
                            time_posted = "N/A"
                            logger.warning("KhÃ´ng tÃ¬m tháº¥y selector cho 'time_posted'") # <-- Log Ä‘á»ƒ debug

                        # DESCRIPTION
                        if details_container:
                            description = details_container.inner_text().strip()
                        else:
                            description = "N/A"
                            logger.warning("KhÃ´ng tÃ¬m tháº¥y selector cho 'description'") # <-- Log Ä‘á»ƒ debug
                        
                        # BÆ°á»›c 3: Ghi dá»¯ liá»‡u
                        job_data = {
                            "title": title,
                            "company": company,
                            "time_posted": time_posted,
                            "description": description,
                            "crawled_at": datetime.now().isoformat(),
                            "processed": False,  # Máº·c Ä‘á»‹nh lÃ  False, sáº½ cáº­p nháº­t sau khi ghi vÃ o DB
                        }
                        
                        scraped_data.append(job_data)
                        logger.info(f"âœ… ÄÃ£ crawl: {title} táº¡i {company}")

                    except Exception as e:
                        # Tá»I Æ¯U LOG Lá»–I: In ra cáº£ URL Ä‘ang bá»‹ lá»—i Ä‘á»ƒ dá»… kiá»ƒm tra láº¡i báº±ng tay
                        current_url = page.url
                        logger.warning(f"âŒ Lá»—i khi xá»­ lÃ½ má»™t job card táº¡i URL: {current_url}. Lá»—i: {e}")
            
                next_button = page.query_selector("a.next-page-button")
                if next_button and next_button.get_attribute("href"):
                    next_url = base_url + next_button.get_attribute("href")
                    logger.info(f"âž¡ï¸ Â Chuyá»ƒn sang trang tiáº¿p theo: {next_url}")
                    page.goto(next_url)
                    page_num += 1
                else:
                    logger.info("âœ… Háº¿t trang, káº¿t thÃºc crawl.")
                    break
                
    except Exception as e:
        logger.error(f"Lá»—i nghiÃªm trá»ng trong quÃ¡ trÃ¬nh crawl: {e}", exc_info=True)
        raise

    if not scraped_data:
        logger.warning("KhÃ´ng crawl Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o.")
        return None


    # logger.info(f"Ghi {len(scraped_data)} jobs vÃ o file {output_file_path}")
    # with open(output_file_path, "w", encoding="utf-8") as f:
    #     json.dump(scraped_data, f, indent=2, ensure_ascii=False)
        
    final_path = save_data_to_destination(
        data_to_save=scraped_data,
        logical_date=kwargs["ds"],
        run_id=kwargs["run_id"]
    )
        
    return final_path

def load_raw_json_to_sql_callable(**kwargs):
    ti = kwargs['ti']
    input_file_path = ti.xcom_pull(task_ids='crawl_and_save_locally_task')
    if not input_file_path:
        logger.warning("KhÃ´ng cÃ³ file Ä‘á»ƒ xá»­ lÃ½. Bá» qua.")
        return

    db_user = os.environ.get("DB_USER", "postgres")
    db_pass = os.environ.get("DB_PASS", "123456")
    db_name = os.environ.get("DB_NAME", "job_db")
    db_host = os.environ.get("DB_HOST", "host.docker.internal")
    db_port = os.environ.get("DB_PORT", "5431")
    db_url = f"postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
    engine = create_engine(db_url)
    
    with open(input_file_path, 'r', encoding='utf-8') as f:
        data_to_load = json.load(f)
    
    if not data_to_load:
        logger.info("File dá»¯ liá»‡u rá»—ng.")
        return

    unique_jobs = { (job.get('title'), job.get('company')): job for job in data_to_load if job.get('title') and job.get('company') }
    deduplicated_data = list(unique_jobs.values())

    if not deduplicated_data:
        logger.info("KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡ sau khi khá»­ trÃ¹ng láº·p.")
        return

    try:
        with engine.connect() as connection:
            logger.info("Kiá»ƒm tra vÃ  táº¡o báº£ng 'jobs' náº¿u cáº§n...")
            METADATA.create_all(engine, checkfirst=True) # Chá»‰ táº¡o báº£ng 'jobs'
            
            insert_stmt = pg_insert(JOBS_TABLE).values(deduplicated_data)
            do_nothing_stmt = insert_stmt.on_conflict_do_nothing(
                index_elements=['title', 'company']
            )
            
            with connection.begin() as transaction:
                result = connection.execute(do_nothing_stmt)
                logger.info(f"âœ… ÄÃ£ chÃ¨n {result.rowcount} jobs thÃ´ má»›i vÃ o database.")
                
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u thÃ´ vÃ o database: {e}")
        raise