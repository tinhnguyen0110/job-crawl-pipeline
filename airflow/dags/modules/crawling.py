import os
import json
import logging
from datetime import datetime
from playwright.sync_api import sync_playwright, Page, TimeoutError as PlaywrightTimeoutError
from sqlalchemy import create_engine
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import SQLAlchemyError
from google.cloud import storage
from .database import METADATA, JOBS_TABLE
from .io_utils import save_data_to_destination
logger = logging.getLogger(__name__)

def _close_popups(page: Page):
    popup_close_selectors = [".dismiss", "button[aria-label='Close']"]
    for selector in popup_close_selectors:
        try:
            popup_button = page.locator(selector).first
            if popup_button.is_visible(timeout=500):
                logger.info(f"Ph√°t hi·ªán v√† ƒë√≥ng pop-up v·ªõi selector: '{selector}'")
                popup_button.click(timeout=1000)
                page.wait_for_timeout(500)
        except PlaywrightTimeoutError:
            pass
        except Exception as e:
            logger.warning(f"L·ªói kh√¥ng mong ƒë·ª£i khi c·ªë g·∫Øng ƒë√≥ng pop-up '{selector}': {e}")

def safe_click(page, job_card):

    _close_popups(page)
    job_card.click()
    _close_popups(page)


def crawl_jobstreet_to_local_callable(**kwargs):
    params = kwargs.get("params", {})
    start_url = params.get("start_url")
    if not start_url:
        raise ValueError("Kh√¥ng t√¨m th·∫•y 'start_url' trong params c·ªßa DAG.")
    
    scraped_data = []

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(start_url)
            base_url = "https://www.jobstreet.vn"
            page_num = 1
            
            while page_num <= 4:
                logger.info(f"üîÑ ƒêang x·ª≠ l√Ω trang {page_num} t·∫°i URL: {page.url}")
                
                try:
                    page.wait_for_selector("div.job-card", timeout=10000)
                    jobs = page.query_selector_all("div.job-card")
                except Exception as e:
                    logger.error(f"Kh√¥ng t√¨m th·∫•y job cards: {e}")
                    break
                
                for job_card in jobs:
                    try:
                        safe_click(page, job_card)
                        # job_card.click() # Ch√∫ng ta s·∫Ω b·ªè force=True v√† x·ª≠ l√Ω pop-up n·∫øu c√≥
                        page.wait_for_selector(".job-description-container", timeout=5000)

                        # B∆∞·ªõc 1: T√¨m c√°c ph·∫ßn t·ª≠ cha
                        header = page.query_selector(".sticky-container")
                        details_container = page.query_selector(".job-description-container")

                        # B∆∞·ªõc 2: T√¨m t·ª´ng ph·∫ßn t·ª≠ con v√† l·∫•y text m·ªôt c√°ch an to√†n
                        # TITLE
                        title_el = header.query_selector(".job-title.heading") if header else None
                        if title_el:
                            title = title_el.inner_text().strip()
                        else:
                            title = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'title'") # <-- Log ƒë·ªÉ debug

                        # COMPANY
                        company_el = header.query_selector(".company") if header else None
                        if company_el:
                            company = company_el.inner_text().strip()
                        else:
                            company = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'company'") # <-- Log ƒë·ªÉ debug

                        # TIME POSTED
                        time_el = header.query_selector(".listed-date") if header else None
                        if time_el:
                            time_posted = time_el.inner_text().strip()
                        else:
                            time_posted = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'time_posted'") # <-- Log ƒë·ªÉ debug

                        # DESCRIPTION
                        if details_container:
                            description = details_container.inner_text().strip()
                        else:
                            description = "N/A"
                            logger.warning("Kh√¥ng t√¨m th·∫•y selector cho 'description'") # <-- Log ƒë·ªÉ debug
                        
                        # B∆∞·ªõc 3: Ghi d·ªØ li·ªáu
                        job_data = {
                            "title": title,
                            "company": company,
                            "time_posted": time_posted,
                            "description": description,
                            "crawled_at": datetime.now().isoformat(),
                            "processed": False,  # M·∫∑c ƒë·ªãnh l√† False, s·∫Ω c·∫≠p nh·∫≠t sau khi ghi v√†o DB
                        }
                        
                        scraped_data.append(job_data)
                        logger.info(f"‚úÖ ƒê√£ crawl: {title} t·∫°i {company}")

                    except Exception as e:
                        # T·ªêI ∆ØU LOG L·ªñI: In ra c·∫£ URL ƒëang b·ªã l·ªói ƒë·ªÉ d·ªÖ ki·ªÉm tra l·∫°i b·∫±ng tay
                        current_url = page.url
                        logger.warning(f"‚ùå L·ªói khi x·ª≠ l√Ω m·ªôt job card t·∫°i URL: {current_url}. L·ªói: {e}")
            
                next_button = page.query_selector("a.next-page-button")
                if next_button and next_button.get_attribute("href"):
                    next_url = base_url + next_button.get_attribute("href")
                    logger.info(f"‚û°Ô∏è ¬†Chuy·ªÉn sang trang ti·∫øp theo: {next_url}")
                    page.goto(next_url)
                    page_num += 1
                else:
                    logger.info("‚úÖ H·∫øt trang, k·∫øt th√∫c crawl.")
                    break
                
    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh crawl: {e}", exc_info=True)
        raise

    if not scraped_data:
        logger.warning("Kh√¥ng crawl ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.")
        return None


    # logger.info(f"Ghi {len(scraped_data)} jobs v√†o file {output_file_path}")
    # with open(output_file_path, "w", encoding="utf-8") as f:
    #     json.dump(scraped_data, f, indent=2, ensure_ascii=False)
        
    final_path = save_data_to_destination(
        data_to_save=scraped_data,
        logical_date=kwargs["ds"],
        run_id=kwargs["run_id"]
    )
        
    return final_path

def _read_data_from_path(file_path: str) -> list:
    """
    H√†m ph·ª• tr·ª£ th√¥ng minh, ƒë·ªçc d·ªØ li·ªáu JSON t·ª´ m·ªôt ƒë∆∞·ªùng d·∫´n b·∫•t k·ª≥.
    ƒê∆∞·ªùng d·∫´n c√≥ th·ªÉ l√† GCS (gs://...) ho·∫∑c local.
    Tr·∫£ v·ªÅ m·ªôt list ch·ª©a d·ªØ li·ªáu.
    """
    logger.info(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ ƒë∆∞·ªùng d·∫´n: {file_path}")
    
    if file_path.startswith('gs://'):
        # K·ªãch b·∫£n ƒë·ªçc t·ª´ Google Cloud Storage
        try:
            client = storage.Client()
            bucket_name, blob_name = file_path.replace("gs://", "").split("/", 1)
            blob = client.bucket(bucket_name).blob(blob_name)
            json_content = blob.download_as_text()
            logger.info("ƒê·ªçc file t·ª´ GCS th√†nh c√¥ng.")
            return json.loads(json_content)
        except Exception as e:
            logger.error(f"L·ªói khi ƒë·ªçc file t·ª´ GCS t·∫°i '{file_path}': {e}")
            raise
    else:
        # K·ªãch b·∫£n ƒë·ªçc t·ª´ local filesystem
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                logger.info("ƒê·ªçc file t·ª´ local th√†nh c√¥ng.")
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n local '{file_path}'.")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"L·ªói parse JSON trong file '{file_path}': {e}")
            raise


def load_raw_json_to_sql_callable(**kwargs):
    ti = kwargs['ti']
    input_file_path = ti.xcom_pull(task_ids='crawl_and_save_locally_task')
    if not input_file_path:
        logger.warning("Kh√¥ng c√≥ file ƒë·ªÉ x·ª≠ l√Ω. B·ªè qua.")
        return

    data_to_load = _read_data_from_path(file_path=input_file_path)
        
    db_user = os.environ.get("DB_USER", "postgres")
    db_pass = os.environ.get("DB_PASS", "123456")
    db_name = os.environ.get("DB_NAME", "job_db")
    db_host = os.environ.get("DB_HOST", "127.0.0.1")
    db_port = os.environ.get("DB_PORT", "5432")
    db_url = f"postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
    engine = create_engine(db_url)
    

    
    if not data_to_load:
        logger.info("File d·ªØ li·ªáu r·ªóng.")
        return

    unique_jobs = { (job.get('title'), job.get('company')): job for job in data_to_load if job.get('title') and job.get('company') }
    deduplicated_data = list(unique_jobs.values())

    if not deduplicated_data:
        logger.info("Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá sau khi kh·ª≠ tr√πng l·∫∑p.")
        return

    try:
        with engine.connect() as connection:
            logger.info("Ki·ªÉm tra v√† t·∫°o b·∫£ng 'jobs' n·∫øu c·∫ßn...")
            METADATA.create_all(engine, checkfirst=True) # Ch·ªâ t·∫°o b·∫£ng 'jobs'
            
            insert_stmt = pg_insert(JOBS_TABLE).values(deduplicated_data)
            do_nothing_stmt = insert_stmt.on_conflict_do_nothing(
                index_elements=['title', 'company']
            )
            
            with connection.begin() as transaction:
                result = connection.execute(do_nothing_stmt)
                logger.info(f"‚úÖ ƒê√£ ch√®n {result.rowcount} jobs th√¥ m·ªõi v√†o database.")
                
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ghi d·ªØ li·ªáu th√¥ v√†o database: {e}")
        raise