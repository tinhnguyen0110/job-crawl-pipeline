# dags/modules/crawling.py

import os

import json

import logging

from datetime import datetime

from playwright.sync_api import sync_playwright



# Thiáº¿t láº­p má»™t logger riÃªng cho module nÃ y

logger = logging.getLogger(__name__)



def crawl_jobstreet_to_local_callable(**kwargs):

Â  Â  """

Â  Â  HÃ m Python nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Airflow gá»i thÃ´ng qua PythonOperator.

Â  Â 

Â  Â  NÃ³ thá»±c hiá»‡n cÃ¡c cÃ´ng viá»‡c sau:

Â  Â  1. Láº¥y cÃ¡c tham sá»‘ (URL, thÆ° má»¥c output) tá»« context cá»§a Airflow.

Â  Â  2. Sá»­ dá»¥ng Playwright Ä‘á»ƒ crawl dá»¯ liá»‡u tá»« JobStreet.

Â  Â  3. Thu tháº­p dá»¯ liá»‡u vÃ o má»™t danh sÃ¡ch (list).

Â  Â  4. Ghi toÃ n bá»™ danh sÃ¡ch vÃ o má»™t file JSON duy nháº¥t trÃªn local filesystem cá»§a worker.

Â  Â  5. Tráº£ vá» Ä‘Æ°á»ng dáº«n cá»§a file Ä‘Ã£ táº¡o Ä‘á»ƒ cÃ¡c task sau cÃ³ thá»ƒ sá»­ dá»¥ng qua XComs.

Â  Â  """

Â  Â  # Láº¥y cÃ¡c tham sá»‘ vÃ  context tá»« Airflow

Â  Â  params = kwargs.get("params", {})

Â  Â  logical_date = kwargs["ds"] # VÃ­ dá»¥: '2025-06-12'

Â  Â  run_id = kwargs["run_id"] Â  # VÃ­ dá»¥: 'scheduled__2025-06-12T00:00:00+00:00'

Â  Â 

Â  Â  start_url = params.get("start_url")

Â  Â  if not start_url:

Â  Â  Â  Â  raise ValueError("KhÃ´ng tÃ¬m tháº¥y 'start_url' trong params cá»§a DAG.")



Â  Â  # ===================================================================

Â  Â  # == Â  Â  Â  Â  Â  Â âœ… THAY Äá»”I DUY NHáº¤T ÄÆ¯á»¢C ÃP Dá»¤NG á» ÄÃ‚Y Â  Â  Â  Â  Â  ==

Â  Â  # ===================================================================

Â  Â  # Tá»‘i Æ°u hÃ³a viá»‡c Ä‘áº·t tÃªn file Ä‘á»ƒ nÃ³ duy nháº¥t cho má»—i láº§n cháº¡y DAG,

Â  Â  # trÃ¡nh xung Ä‘á»™t khi cháº¡y láº¡i (retry) hoáº·c cháº¡y song song.



Â  Â  # Táº¡o má»™t tÃªn file an toÃ n tá»« run_id (thay tháº¿ cÃ¡c kÃ½ tá»± khÃ´ng há»£p lá»‡)

Â  Â  safe_run_id = run_id.replace(":", "_").replace("+", "_")

Â  Â 

Â  Â  # Táº¡o thÆ° má»¥c con dá»±a trÃªn ngÃ y cháº¡y Ä‘á»ƒ dá»… quáº£n lÃ½ file

Â  Â  output_dir = f"/opt/airflow/data/{logical_date}"

Â  Â  output_filename = f"{safe_run_id}.json" # TÃªn file giá» lÃ  run_id Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch

Â  Â  output_file_path = os.path.join(output_dir, output_filename)

Â  Â  # ===================================================================



Â  Â  logger.info(f"ğŸš€ Báº¯t Ä‘áº§u crawl tá»« URL: {start_url}")

Â  Â  logger.info(f"ğŸ“ File output sáº½ Ä‘Æ°á»£c lÆ°u táº¡i: {output_file_path}")



Â  Â  # Äáº£m báº£o thÆ° má»¥c output tá»“n táº¡i

Â  Â  os.makedirs(output_dir, exist_ok=True)



Â  Â  scraped_data = [] # NÆ¡i lÆ°u trá»¯ táº¡m thá»i dá»¯ liá»‡u crawl Ä‘Æ°á»£c



Â  Â  try:

Â  Â  Â  Â  with sync_playwright() as p:

Â  Â  Â  Â  Â  Â  browser = p.chromium.launch(headless=True)

Â  Â  Â  Â  Â  Â  page = browser.new_page()

Â  Â  Â  Â  Â  Â  page.goto(start_url)

Â  Â  Â  Â  Â  Â 

Â  Â  Â  Â  Â  Â  base_url = "https://www.jobstreet.vn"

Â  Â  Â  Â  Â  Â  page_num = 1

Â  Â  Â  Â  Â  Â  # Giá»›i háº¡n crawl 2 trang Ä‘á»ƒ test nhanh, báº¡n cÃ³ thá»ƒ bá» giá»›i háº¡n nÃ y

Â  Â  Â  Â  Â  Â  while page_num <= 2:

Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”„ Äang xá»­ lÃ½ trang {page_num}...")

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("Äang tÃ¬m kiáº¿m job cards trÃªn trang...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.wait_for_selector("div.job-card", timeout=3000)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  jobs_on_page = page.query_selector_all("div.job-card")

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(e)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y job cards trÃªn trang {page.url}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  Â  Â  Â  Â  for job_card in jobs_on_page:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  job_card.click(force=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.wait_for_selector(".job-description-container", timeout=5000)



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  header = page.query_selector(".sticky-container")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title = header.query_selector(".job-title.heading").inner_text().strip() if header else "N/A"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company = header.query_selector(".company").inner_text().strip() if header else "N/A"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time_posted = header.query_selector(".listed-date").inner_text().strip() if header else "N/A"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  description = page.query_selector(".job-description-container").inner_text().strip()



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  job_data = {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "title": title,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "company": company,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "time_posted": time_posted,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "description": description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "crawled_at": datetime.now().isoformat()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scraped_data.append(job_data)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… ÄÃ£ crawl: {title} táº¡i {company}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âŒ Lá»—i khi xá»­ lÃ½ má»™t job card: {e}")



Â  Â  Â  Â  Â  Â  Â  Â  # Äiá»u hÆ°á»›ng sang trang tiáº¿p theo

Â  Â  Â  Â  Â  Â  Â  Â  next_button = page.query_selector("a.next-page-button")

Â  Â  Â  Â  Â  Â  Â  Â  if next_button and next_button.get_attribute("href"):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  next_url = base_url + next_button.get_attribute("href")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"â¡ï¸ Â Chuyá»ƒn sang trang tiáº¿p theo: {next_url}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page.goto(next_url)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page_num += 1

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info("âœ… Háº¿t trang, káº¿t thÃºc crawl.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â 

Â  Â  Â  Â  Â  Â  browser.close()



Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"Lá»—i nghiÃªm trá»ng trong quÃ¡ trÃ¬nh crawl: {e}")

Â  Â  Â  Â  # NÃ©m lá»—i ra ngoÃ i Ä‘á»ƒ Airflow biáº¿t task nÃ y Ä‘Ã£ tháº¥t báº¡i

Â  Â  Â  Â  raise



Â  Â  if not scraped_data:

Â  Â  Â  Â  logger.warning("KhÃ´ng crawl Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o.")

Â  Â  Â  Â  return None



Â  Â  # Ghi dá»¯ liá»‡u vÃ o file JSON

Â  Â  logger.info(f"Ghi {len(scraped_data)} jobs vÃ o file {output_file_path}")

Â  Â  with open(output_file_path, "w", encoding="utf-8") as f:

Â  Â  Â  Â  json.dump(scraped_data, f, indent=2, ensure_ascii=False)



Â  Â  # Tráº£ vá» Ä‘Æ°á»ng dáº«n file, Airflow sáº½ tá»± Ä‘á»™ng lÆ°u vÃ o XComs

Â  Â  return output_file_path