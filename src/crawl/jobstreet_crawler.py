from playwright.sync_api import sync_playwright
from config.config_loader import load_config
import csv
import os
from datetime import datetime
from utils.logger import get_logger

logger = get_logger("crawler.jobstreet")
logger.info("üöÄ B·∫Øt ƒë·∫ßu crawl JobStreet AI Jobs")

def crawl_jobstreet_ai_jobs():
    config = load_config()
    base_url = "https://www.jobstreet.vn"
    start_url = config["crawling"]["url"]
    csv_file = config["crawling"]["csv_output_path"]
    file_exists = os.path.isfile(csv_file)

    os.makedirs(os.path.dirname(csv_file), exist_ok=True)
    with open(csv_file, mode="a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["title", "company", "description", "time_posted", "date_crawled"])
        if not file_exists:
            writer.writeheader()

        with sync_playwright() as p:
            # browser = p.chromium.launch(
            #     headless=config["crawling"].get("headless", False),
            #     slow_mo=config["crawling"].get("slow_mo", 100)
            # )
            browser = p.chromium.launch(
                headless=True,
            )
            page = browser.new_page()
            page.goto(start_url)
            logger.info(f"ƒêang truy c·∫≠p trang: {start_url}")

            page_num = 1
            while True:
                logger.info(f"\nüîÑ Trang {page_num}...\n")
                
                try:
                    page.wait_for_selector("div.job-card", timeout=10000)
                    jobs = page.query_selector_all("div.job-card")
                except Exception as e:
                    logger.error(f"Kh√¥ng t√¨m th·∫•y job cards: {e}")
                    break
                
                for idx, job in enumerate(jobs):
                    try:
                        job.click()
                        page.wait_for_selector(".job-description-container", timeout=10000)

                        header = page.query_selector(".sticky-container")
                        title_el = header.query_selector(".job-title.heading") if header else None
                        company_el = header.query_selector(".company") if header else None
                        time_el = header.query_selector(".listed-date") if header else None
                        details_el = page.query_selector(".job-description-container")

                        title = title_el.inner_text().strip() if title_el else "Kh√¥ng r√µ"
                        company = company_el.inner_text().strip() if company_el else "Kh√¥ng r√µ"
                        time_posted = time_el.inner_text().strip() if time_el else "Kh√¥ng r√µ"
                        description = details_el.inner_text().strip() if details_el else "Kh√¥ng r√µ"

                        writer.writerow({
                            "title": title,
                            "company": company,
                            "description": description,
                            "time_posted": time_posted,
                            "date_crawled": datetime.now().date().isoformat()
                        })

                        logger.debug(f"‚úÖ [{idx+1}] {title} - {company}")
                        page.wait_for_timeout(300)

                    except Exception as e:
                        logger.warning(f"‚ùå L·ªói job {idx+1}: {e}")

                next_button = page.query_selector("a.next-page-button")
                if next_button:
                    next_href = next_button.get_attribute("href")
                    if not next_href:
                        logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y href c·ªßa trang k·∫ø ti·∫øp.")
                        break
                    next_url = base_url + next_href
                    logger.info(f"‚û°Ô∏è Chuy·ªÉn sang: {next_url}")
                    page.goto(next_url)
                    try:
                        page.wait_for_selector(".dismiss", timeout=3000)
                        page.click(".dismiss")
                    except:
                        pass
                    page.wait_for_load_state("load")
                    page_num += 1
                    if page_num == 3:   
                        logger.info("‚ö†Ô∏è D·ª´ng l·∫°i sau 2 trang ƒë·ªÉ tr√°nh qu√° t·∫£i.")
                        break
                else:
                    logger.info("\n‚úÖ H·∫øt trang.")
                    break

            browser.close()
    logger.info(f"\nüìÅ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o {csv_file}")

if __name__ == "__main__":
    crawl_jobstreet_ai_jobs()