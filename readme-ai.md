<div id="top">

<!-- HEADER STYLE: CLASSIC -->
<div align="center">

<img src="readmeai/assets/logos/purple.svg" width="30%" style="position: relative; top: 0; right: 0;" alt="Project Logo"/>

# CRAWL2INSIGHT_2

<em>Unleash AI Insights at Scale!</em>

<!-- BADGES -->
<!-- local repository, no metadata badges. -->

<em>Built with the tools and technologies:</em>

<img src="https://img.shields.io/badge/Redis-FF4438.svg?style=default&logo=Redis&logoColor=white" alt="Redis">
<img src="https://img.shields.io/badge/Docker-2496ED.svg?style=default&logo=Docker&logoColor=white" alt="Docker">
<img src="https://img.shields.io/badge/Python-3776AB.svg?style=default&logo=Python&logoColor=white" alt="Python">
<img src="https://img.shields.io/badge/pandas-150458.svg?style=default&logo=pandas&logoColor=white" alt="pandas">
<img src="https://img.shields.io/badge/YAML-CB171E.svg?style=default&logo=YAML&logoColor=white" alt="YAML">

</div>
<br>

---

## Table of Contents

- [Table of Contents](#table-of-contents)
- [Overview](#overview)
- [Features](#features)
- [Project Structure](#project-structure)
    - [Project Index](#project-index)
- [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
    - [Usage](#usage)
    - [Testing](#testing)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## Overview

A comprehensive data extraction and analysis tool for AI Engineers, designed to streamline job searching and deliver actionable insights. The core features include:

   - **ğŸ” Job Crawler:** Automates the process of gathering AI job listings from JobStreet Vietnam.
   - **ğŸ§© Data Processing Pipeline:** Orchestrates data transformation, analysis, and storage for efficient querying and insights.
   - **ğŸ“Š Interactive Job Browser:** Allows users to filter jobs by location and seniority level, with an intuitive interface displaying job details.
   - **ğŸ’» Dockerized Apache Airflow Setup:** Simplifies local development with a customizable Docker Compose setup for an Apache Airflow cluster.
   - **ğŸ”Œ Dependency Management:** Seamless installation and management of essential libraries using both `requirements.txt` and `Airflow requirements.txt`.

---

## Features

|      | Component       | Details                              |
| :--- | :-------------- | :----------------------------------- |
| âš™ï¸  | **Architecture**  | <ul><li>Microservices architecture with Docker containers.</li><li>Airflow pipeline for data processing and scheduling.</li></ul> |
| ğŸ”© | **Code Quality**  | <ul><li>Well-structured codebase, following PEP8 style guide.</li><li>Use of type hints and docstrings for clarity.</li></ul> |
| ğŸ“„ | **Documentation** | <ul><li>Docker documentation provided in `docker-compose.yaml` and Airflow's Dockerfile.</li><li>Config file (config.yaml) for configuration settings.</li></ul> |
| ğŸ”Œ | **Integrations**  | <ul><li>Integration with PostgreSQL database, Redis cache, and various APIs using `requests` library.</li><li>Airflow integrates with other Airflow components like Airflow-worker, Airflow-scheduler, Flower, and Airflow-webserver.</li></ul> |
| ğŸ§© | **Modularity**    | <ul><li>Modular design with separate modules for different tasks (e.g., data extraction, data processing, data storage).</li><li>Use of functions and classes to encapsulate functionality.</li></ul> |
| ğŸ§ª | **Testing**       | <ul><li>Unit tests using Python's built-in `unittest` module.</li><li>Integration tests using Airflow's testing framework.</li></ul> |
| âš¡ï¸  | **Performance**   | <ul><li>Optimized for efficient data processing with the use of libraries like `pandas` and `dateparser`.</li><li>Asynchronous tasks using Airflow to improve performance.</li></ul> |
| ğŸ›¡ï¸ | **Security**      | <ul><li>Use of secure connections (HTTPS) for API requests.</li><li>Database connection details stored in environment variables instead of hardcoding them.</li></ul> |
| ğŸ“¦ | **Dependencies**  | <ul><li>Python dependencies listed in `requirements.txt` and Airflow's `requirements.txt`.</li><li>Docker-compose files for managing containers.</li></ul> |

---

## Project Structure

```sh
â””â”€â”€ Crawl2Insight_2/
    â”œâ”€â”€ airflow
    â”‚   â”œâ”€â”€ dags
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ logs
    â”‚   â””â”€â”€ requirements.txt
    â”œâ”€â”€ app
    â”‚   â””â”€â”€ job_browser.py
    â”œâ”€â”€ data
    â”‚   â”œâ”€â”€ processed
    â”‚   â””â”€â”€ raw
    â”œâ”€â”€ db
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â””â”€â”€ init_db.py
    â”œâ”€â”€ Docker
    â”‚   â”œâ”€â”€ docker-compose.postgres.yaml
    â”‚   â””â”€â”€ scripts
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ logs
    â”‚   â””â”€â”€ pipeline.log
    â”œâ”€â”€ README.md
    â”œâ”€â”€ readmeai-offline.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ src
    â”‚   â”œâ”€â”€ config
    â”‚   â”œâ”€â”€ crawl
    â”‚   â”œâ”€â”€ ingestion
    â”‚   â”œâ”€â”€ pipeline
    â”‚   â”œâ”€â”€ processing
    â”‚   â””â”€â”€ utils
    â”œâ”€â”€ test
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â””â”€â”€ test_jobstreet_live.py
    â””â”€â”€ testgit.txt
```

### Project Index

<details open>
	<summary><b><code>D:\DATA\CRAWL2INSIGHT_2/</code></b></summary>
	<!-- __root__ Submodule -->
	<details>
		<summary><b>__root__</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ __root__</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/docker-compose.yaml'>docker-compose.yaml</a></b></td>
					<td style='padding: 8px;'>- Configures a Docker Compose setup for an Apache Airflow cluster using CeleryExecutor, Redis, and PostgreSQL<br>- This setup is designed for local development purposes, not production deployment<br>- It supports customization through environment variables or an.env file<br>- The configuration enables basic health checks, scheduler, webserver, worker, triggerer, and CLI access to Airflow.</td>
				</tr>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/requirements.txt'>requirements.txt</a></b></td>
					<td style='padding: 8px;'>- Manages project dependencies by listing essential libraries such as requests, pandas, and dateparser<br>- Ensures seamless installation of these packages when the project is set up or updated, facilitating efficient data processing and web service interactions.</td>
				</tr>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/testgit.txt'>testgit.txt</a></b></td>
					<td style='padding: 8px;'>- Data\Crawl2Insight_2)<br>- This pipeline automates the process of fetching job data from various sources, transforming it, and delivering insights.</td>
				</tr>
			</table>
		</blockquote>
	</details>
	<!-- airflow Submodule -->
	<details>
		<summary><b>airflow</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ airflow</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/airflow\Dockerfile'>Dockerfile</a></b></td>
					<td style='padding: 8px;'>- This Dockerfile configures an Apache Airflow environment with Python 3.10 and specific dependencies, including Playwright<br>- It sets up the environment to run smoothly within a container, ensuring seamless execution of data pipelines in the larger Airflow project structure.</td>
				</tr>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/airflow\requirements.txt'>requirements.txt</a></b></td>
					<td style='padding: 8px;'>- Manages project dependencies using Airflow, a platform to programmatically schedule and monitor workflows<br>- Includes essential libraries such as Playwright for web automation, Requests for HTTP requests, Pandas for data manipulation, Psycopg2 for PostgreSQL database interaction, and Dateparser for parsing date strings.</td>
				</tr>
			</table>
			<!-- dags Submodule -->
			<details>
				<summary><b>dags</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ airflow.dags</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/airflow\dags\crawl2insight_dag.py'>crawl2insight_dag.py</a></b></td>
							<td style='padding: 8px;'>- This Python script, located within the Airflow DAG structure, triggers a daily data pipeline execution<br>- The pipeline is defined in another file (/opt/airflow/src/pipeline/run_pipeline.py), which is called using the BashOperator from the Airflow library<br>- In essence, this script schedules the execution of a larger data processing workflow on a daily basis within an Airflow environment.</td>
						</tr>
					</table>
				</blockquote>
			</details>
		</blockquote>
	</details>
	<!-- app Submodule -->
	<details>
		<summary><b>app</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ app</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/app\job_browser.py'>job_browser.py</a></b></td>
					<td style='padding: 8px;'>- This Python script, <code>app/job_browser.py</code>, serves as a job explorer application using Streamlit<br>- It fetches and displays job listings from a PostgreSQL database, allowing users to filter jobs by location and seniority level<br>- The filtered results are presented in an interactive interface, providing details such as job title, company, date posted, salary, requirements, benefits, and a detailed description<br>- This tool aims to simplify the process of job searching for AI Engineers.</td>
				</tr>
			</table>
		</blockquote>
	</details>
	<!-- db Submodule -->
	<details>
		<summary><b>db</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ db</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/db\init_db.py'>init_db.py</a></b></td>
					<td style='padding: 8px;'>- Initializes a database for a job crawler application, creating two tables (raw_jobs and structured_jobs) to store job listings and their details<br>- This setup enables the efficient storage and processing of job data for further analysis or search functionality.</td>
				</tr>
			</table>
		</blockquote>
	</details>
	<!-- Docker Submodule -->
	<details>
		<summary><b>Docker</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ Docker</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/Docker\docker-compose.postgres.yaml'>docker-compose.postgres.yaml</a></b></td>
					<td style='padding: 8px;'>- Configures a persistent PostgreSQL database instance using Docker Compose, providing an accessible and secure data storage solution for the projects application<br>- The specified environment variables set up the necessary database configuration, while the volume ensures data persistence across container restarts.</td>
				</tr>
			</table>
		</blockquote>
	</details>
	<!-- src Submodule -->
	<details>
		<summary><b>src</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ src</b></code>
			<!-- config Submodule -->
			<details>
				<summary><b>config</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ src.config</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\config\config.yaml'>config.yaml</a></b></td>
							<td style='padding: 8px;'>- Configures Crawl2Insight, a data extraction tool that gathers AI job listings from JobStreet Vietnam and processes them using Ollamas Mistral model<br>- The extracted data is saved in CSV format, with structured output stored separately<br>- Database connection details are specified for storing the processed data in a PostgreSQL database.</td>
						</tr>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\config\config_loader.py'>config_loader.py</a></b></td>
							<td style='padding: 8px;'>- The <code>config_loader.py</code> module serves as the central configuration access point within the project architecture<br>- It reads and parses a YAML file named config.yaml located in the src/config directory, thereby providing a structured and easily modifiable configuration interface for the entire application.</td>
						</tr>
					</table>
				</blockquote>
			</details>
			<!-- crawl Submodule -->
			<details>
				<summary><b>crawl</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ src.crawl</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\crawl\jobstreet_crawler.py'>jobstreet_crawler.py</a></b></td>
							<td style='padding: 8px;'>- This Python script is a web scraper designed to crawl AI job listings from JobStreet Vietnam<br>- It navigates through multiple pages, extracts job details such as title, company, description, time posted, and date crawled, and writes the data into a CSV file<br>- The script utilizes Playwright for browser automation and configures its settings based on a configuration file.</td>
						</tr>
					</table>
				</blockquote>
			</details>
			<!-- ingestion Submodule -->
			<details>
				<summary><b>ingestion</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ src.ingestion</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\ingestion\insert_raw_jobs.py'>insert_raw_jobs.py</a></b></td>
							<td style='padding: 8px;'>- This Python script, located within the ingestion module, is designed to import job listings from a CSV file into a PostgreSQL database<br>- It reads the configuration file to determine the database connection details and the location of the CSV output file<br>- If the CSV file exists, it processes each row, preparing and executing an SQL INSERT statement to add the job listing data to the raw_jobs table in the database<br>- The script handles potential conflicts by ignoring duplicate entries based on title, company, and date crawled.</td>
						</tr>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\ingestion\insert_structured_jobs.py'>insert_structured_jobs.py</a></b></td>
							<td style='padding: 8px;'>- This Python script, <code>insert_structured_jobs.py</code>, is designed to ingest and update structured job data from a CSV file into a database<br>- It reads the configuration for the database connection and the location of the CSV file, validates the CSV file, and then processes each row by inserting or updating the corresponding record in the structured_jobs table<br>- This process ensures that the job data is consistently structured and up-to-date within the database.</td>
						</tr>
					</table>
				</blockquote>
			</details>
			<!-- pipeline Submodule -->
			<details>
				<summary><b>pipeline</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ src.pipeline</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\pipeline\run_pipeline.py'>run_pipeline.py</a></b></td>
							<td style='padding: 8px;'>- Crawling AI job listings from JobStreet, importing raw data into a database, processing the raw data, structuring the processed data, and finally inserting the structured data into the database<br>- The goal is to automate the process of gathering, preparing, and storing job listings for efficient querying and analysis.</td>
						</tr>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\pipeline\__main__.py'>__main__.py</a></b></td>
							<td style='padding: 8px;'>- The <code>src\pipeline\__main__.py</code> file serves as the entry point to execute the entire data pipeline defined in the <code>src/pipeline</code> directory<br>- This script initiates the main function, which orchestrates the flow of data transformation and analysis tasks, ultimately delivering actionable insights from raw data.</td>
						</tr>
					</table>
				</blockquote>
			</details>
			<!-- processing Submodule -->
			<details>
				<summary><b>processing</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ src.processing</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\processing\process_raw_jobs.py'>process_raw_jobs.py</a></b></td>
							<td style='padding: 8px;'>- This Python script, located at <code>src\processing\process_raw_jobs.py</code>, extracts and structures job data from a raw database<br>- It uses an external API (Ollama) to parse job titles, descriptions, locations, salaries, requirements, benefits, and post dates<br>- The processed data is then saved as a CSV file for further analysis<br>- This tool helps in efficiently organizing and analyzing job listings for various purposes.</td>
						</tr>
					</table>
				</blockquote>
			</details>
			<!-- utils Submodule -->
			<details>
				<summary><b>utils</b></summary>
				<blockquote>
					<div class='directory-path' style='padding: 8px 0; color: #666;'>
						<code><b>â¦¿ src.utils</b></code>
					<table style='width: 100%; border-collapse: collapse;'>
					<thead>
						<tr style='background-color: #f8f9fa;'>
							<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
							<th style='text-align: left; padding: 8px;'>Summary</th>
						</tr>
					</thead>
						<tr style='border-bottom: 1px solid #eee;'>
							<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/src\utils\logger.py'>logger.py</a></b></td>
							<td style='padding: 8px;'>Provides a customizable logging utility to manage and format log messages across the application, directing logs to stdout for real-time monitoring and debugging purposes.</td>
						</tr>
					</table>
				</blockquote>
			</details>
		</blockquote>
	</details>
	<!-- test Submodule -->
	<details>
		<summary><b>test</b></summary>
		<blockquote>
			<div class='directory-path' style='padding: 8px 0; color: #666;'>
				<code><b>â¦¿ test</b></code>
			<table style='width: 100%; border-collapse: collapse;'>
			<thead>
				<tr style='background-color: #f8f9fa;'>
					<th style='width: 30%; text-align: left; padding: 8px;'>File Name</th>
					<th style='text-align: left; padding: 8px;'>Summary</th>
				</tr>
			</thead>
				<tr style='border-bottom: 1px solid #eee;'>
					<td style='padding: 8px;'><b><a href='D:\Data\Crawl2Insight_2/blob/master/test\test_jobstreet_live.py'>test_jobstreet_live.py</a></b></td>
					<td style='padding: 8px;'>- This script initiates a test for the jobstreet_crawler module within the provided project structure<br>- It verifies that the specified crawling function successfully scrapes AI jobs from JobStreet and saves the results in a CSV file, ensuring at least one job is recorded with title and description fields.</td>
				</tr>
			</table>
		</blockquote>
	</details>
</details>

---

## Getting Started

### Prerequisites

This project requires the following dependencies:

- **Programming Language:** Python
- **Package Manager:** Pip
- **Container Runtime:** Docker

### Installation

Build Crawl2Insight_2 from the source and intsall dependencies:

1. **Clone the repository:**

    ```sh
    â¯ git clone ../Crawl2Insight_2
    ```

2. **Navigate to the project directory:**

    ```sh
    â¯ cd Crawl2Insight_2
    ```

3. **Install the dependencies:**

<!-- SHIELDS BADGE CURRENTLY DISABLED -->
	<!-- [![docker][docker-shield]][docker-link] -->
	<!-- REFERENCE LINKS -->
	<!-- [docker-shield]: https://img.shields.io/badge/Docker-2CA5E0.svg?style={badge_style}&logo=docker&logoColor=white -->
	<!-- [docker-link]: https://www.docker.com/ -->

	**Using [docker](https://www.docker.com/):**

	```sh
	â¯ docker build -t Data/Crawl2Insight_2 .
	```
<!-- SHIELDS BADGE CURRENTLY DISABLED -->
	<!-- [![pip][pip-shield]][pip-link] -->
	<!-- REFERENCE LINKS -->
	<!-- [pip-shield]: https://img.shields.io/badge/Pip-3776AB.svg?style={badge_style}&logo=pypi&logoColor=white -->
	<!-- [pip-link]: https://pypi.org/project/pip/ -->

	**Using [pip](https://pypi.org/project/pip/):**

	```sh
	â¯ pip install -r requirements.txt, airflow\requirements.txt
	```

### Usage

Run the project with:

**Using [docker](https://www.docker.com/):**
```sh
docker run -it {image_name}
```
**Using [pip](https://pypi.org/project/pip/):**
```sh
python {entrypoint}
```

### Testing

Crawl2insight_2 uses the {__test_framework__} test framework. Run the test suite with:

**Using [pip](https://pypi.org/project/pip/):**
```sh
pytest
```

---

## Roadmap

- [X] **`Task 1`**: <strike>Implement feature one.</strike>
- [ ] **`Task 2`**: Implement feature two.
- [ ] **`Task 3`**: Implement feature three.

---

## Contributing

- **ğŸ’¬ [Join the Discussions](https://LOCAL/Data/Crawl2Insight_2/discussions)**: Share your insights, provide feedback, or ask questions.
- **ğŸ› [Report Issues](https://LOCAL/Data/Crawl2Insight_2/issues)**: Submit bugs found or log feature requests for the `Crawl2Insight_2` project.
- **ğŸ’¡ [Submit Pull Requests](https://LOCAL/Data/Crawl2Insight_2/blob/main/CONTRIBUTING.md)**: Review open PRs, and submit your own PRs.

<details closed>
<summary>Contributing Guidelines</summary>

1. **Fork the Repository**: Start by forking the project repository to your LOCAL account.
2. **Clone Locally**: Clone the forked repository to your local machine using a git client.
   ```sh
   git clone D:\Data\Crawl2Insight_2
   ```
3. **Create a New Branch**: Always work on a new branch, giving it a descriptive name.
   ```sh
   git checkout -b new-feature-x
   ```
4. **Make Your Changes**: Develop and test your changes locally.
5. **Commit Your Changes**: Commit with a clear message describing your updates.
   ```sh
   git commit -m 'Implemented new feature x.'
   ```
6. **Push to LOCAL**: Push the changes to your forked repository.
   ```sh
   git push origin new-feature-x
   ```
7. **Submit a Pull Request**: Create a PR against the original project repository. Clearly describe the changes and their motivations.
8. **Review**: Once your PR is reviewed and approved, it will be merged into the main branch. Congratulations on your contribution!
</details>

<details closed>
<summary>Contributor Graph</summary>
<br>
<p align="left">
   <a href="https://LOCAL{/Data/Crawl2Insight_2/}graphs/contributors">
      <img src="https://contrib.rocks/image?repo=Data/Crawl2Insight_2">
   </a>
</p>
</details>

---

## License

Crawl2insight_2 is protected under the [LICENSE](https://choosealicense.com/licenses) License. For more details, refer to the [LICENSE](https://choosealicense.com/licenses/) file.

---

## Acknowledgments

- Credit `contributors`, `inspiration`, `references`, etc.

<div align="right">

[![][back-to-top]](#top)

</div>


[back-to-top]: https://img.shields.io/badge/-BACK_TO_TOP-151515?style=flat-square


---
